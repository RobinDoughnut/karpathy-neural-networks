{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2149363b-3acf-4c9b-bd02-729c556c35a2",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5fd9ea-e86b-48cd-a050-a2ae75f4231a",
   "metadata": {},
   "source": [
    "**Task:** Train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e0ee9930-277e-4b00-b06d-28948eaf8bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import notebook\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1b82f80c-85e2-4f55-9d75-4e1dc8883579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "52c45bb8-3b5b-4ab3-a991-4570d669d623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of names: 32,033\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of names: {len(words):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0d03c71b-81e2-4eb5-b40d-2b0af0cb2137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First ten words: ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n"
     ]
    }
   ],
   "source": [
    "print(f\"First ten words: {words[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "655be8fe-9594-4430-8732-2dcf4376ac02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of words: 32033\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of words: {len(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a9ca50f4-698e-468c-9369-4246a00566cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of minimum and maximum words: 2 and max 15\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of minimum and maximum words: {min(len(w) for (w) in words)} and max {max(len(w) for (w) in words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8345f1e1-7899-4a12-a332-0169ec310e4e",
   "metadata": {},
   "source": [
    "## Use MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b45212c4-1e97-4a3c-913d-16e72526c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of characters:\n",
    "\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "chars = [\".\"] + chars\n",
    "\n",
    "# # make a dictionary of character to index\n",
    "stoi = {ch: i for (i, ch) in enumerate(chars)}\n",
    "\n",
    "# # make a dictionary of index to character\n",
    "itos = {i: ch for (ch, i) in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1b2243b4-13c4-475f-bed9-9e950c935a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "        # add full stop at start and end\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "\n",
    "        xs.append([ix1, ix2])\n",
    "        ys.append(ix3)\n",
    "\n",
    "xs = torch.tensor(xs, dtype=torch.int64)\n",
    "ys = torch.tensor(ys, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ff184f38-96b2-40f1-9fe5-fcdc3bdcef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((27*2,27), requires_grad =True, device = device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "383988b1-a1e9-4c5b-a791-f9b483640865",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# if k % 10 == 0:\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#     print(f\"{k}: {loss.item():.4f}\")\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n\u001b[1;32m     20\u001b[0m W\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# update weights\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:340\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    331\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    332\u001b[0m     (inputs,)\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m    337\u001b[0m )\n\u001b[1;32m    339\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 340\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:198\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    196\u001b[0m     out_numel_is_1 \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_numel_is_1:\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_dtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[1;32m    202\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "for k in range(200):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes = 27).float().to(device)\n",
    "    xenc = xenc.view(-1, 27*2)\n",
    "\n",
    "    # do softmax\n",
    "    logits = xenc @ W\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / counts.sum(dim = 1, keepdim = True)\n",
    "\n",
    "    # loss(normalized negative log likelihood)\n",
    "    loss = - probs[torch.arange(len(xs)), ys]\n",
    "    # add regularization\n",
    "    loss += 0.2 * W.pow(2).mean()\n",
    "\n",
    "    # if k % 10 == 0:\n",
    "    #     print(f\"{k}: {loss.item():.4f}\")\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        W -= 50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6429ccac-ec52-4b83-9095-749ec78a953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 4.5282\n",
      "10: 2.5694\n",
      "20: 2.4515\n",
      "30: 2.4122\n",
      "40: 2.3933\n",
      "50: 2.3827\n",
      "60: 2.3762\n",
      "70: 2.3721\n",
      "80: 2.3695\n",
      "90: 2.3677\n",
      "100: 2.3665\n",
      "110: 2.3657\n",
      "120: 2.3651\n",
      "130: 2.3647\n",
      "140: 2.3644\n",
      "150: 2.3642\n",
      "160: 2.3641\n",
      "170: 2.3640\n",
      "180: 2.3639\n",
      "190: 2.3639\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((27*2,27), requires_grad = True, device = device)\n",
    "for k in range(200):\n",
    "    # do forward pass\n",
    "    xenc = F.one_hot(xs, num_classes = 27).float().to(device)\n",
    "    xenc = xenc.view(-1, 27*2)\n",
    "    \n",
    "    # do softmax\n",
    "    logits = xenc @ W\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / counts.sum(dim = 1, keepdim = True)\n",
    "    \n",
    "    # loss(normalized negative log likelihood)\n",
    "    loss = - probs[torch.arange(len(xs)), ys].log().mean()\n",
    "    # add regularization\n",
    "    loss += 0.2 * W.pow(2).mean()\n",
    "\n",
    "    if k % 10 == 0:\n",
    "        print(f\"{k}: {loss.item():.4f}\")\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        W -= 50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a44f5b5-50ae-48cb-89cb-c9abd9a40a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_loss(input_list, verbose = False):\n",
    "    log_likelihood = 0.0\n",
    "    n = 0\n",
    "    for w in input_list:\n",
    "        chs = [\".\"] + list(w) + [\".\"]\n",
    "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            ix1 = stoi[ch1]\n",
    "            ix2 = stoi[ch2]\n",
    "            ix3 = stoi[ch3]\n",
    "\n",
    "            prob = P[ix1, ix2, ix3]\n",
    "            logprob = torch.log(prob)\n",
    "            log_likelihood += logprob\n",
    "            n += 1\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"{ch1}{ch2} -> {prob:.4f} {logprob:.4f}\")\n",
    "\n",
    "    # higher the log likelihood (closer to 0) is better\n",
    "    print(f\"log Likelihood: {log_likelihood}\")\n",
    "\n",
    "    # but in loss function lower is better, so we negate it\n",
    "    nll = -log_likelihood\n",
    "    print(f\"Negative log likelihood: {nll}\")\n",
    "\n",
    "    # normalize it\n",
    "    print(f\"Normalized Negative log Likelihood: {(nll / n)}\") # we need to minimize this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef5cf92-4ba5-4359-b131-9b49c2095bf8",
   "metadata": {},
   "source": [
    "## Make loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a148b37c-c638-49df-a544-e664dad9ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.ones(27, 27, 27, dtype = torch.int32, device = device)\n",
    "N[0, 0, 0] = 0\n",
    "# getting the Bigrams\n",
    "for w in words:\n",
    "    # add start and end tokens\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "\n",
    "        N[ix1, ix2, ix3] += 1\n",
    "\n",
    "P = N / N.sum(dim = 2, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "70b4bc81-e948-4e5f-830c-60a8ef55d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_loss(input_list, verbose = False):\n",
    "    log_likelihood = 0.0\n",
    "    n = 0\n",
    "    for w in input_list:\n",
    "        chs = [\".\"] + list(w) + [\".\"]\n",
    "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            ix1 = stoi[ch1]\n",
    "            ix2 = stoi[ch2]\n",
    "            ix3 = stoi[ch3]\n",
    "\n",
    "            prob = P[ix1, ix2, ix3]\n",
    "            logprob = torch.log(prob)\n",
    "            log_likelihood += logprob\n",
    "            n += 1\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"{ch1}{ch2} -> {prob:.4f} {logprob:.4f}\")\n",
    "\n",
    "    # higher the log likelihood (closer to 0) is better\n",
    "    print(f\"log Likelihood: {log_likelihood}\")\n",
    "\n",
    "    # but in loss function lower is better, so we negate it\n",
    "    nll = -log_likelihood\n",
    "    print(f\"Negative log likelihood: {nll}\")\n",
    "\n",
    "    # normalize it\n",
    "    print(f\"Normalized Negative log Likelihood: {(nll / n)}\") # we need to minimize this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1637f734-5d02-4156-901e-0b23b823e686",
   "metadata": {},
   "source": [
    "### Sample from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "aac22404-d5a9-495e-8435-cc6720fc924c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elxinzlayshrisekranya.\n",
      "pn.\n",
      "zoan.\n",
      "uttny.\n",
      "ad.\n",
      "os.\n",
      "eria.\n",
      "ai.\n",
      "raw.\n",
      "myranseonn.\n",
      "log Likelihood: -183.25955200195312\n",
      "Negative log likelihood: 183.25955200195312\n",
      "Normalized Negative log Likelihood: 2.8193776607513428\n"
     ]
    }
   ],
   "source": [
    "names = []\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    ix1, ix2 = 0, 0\n",
    "    while True:\n",
    "        # previosly we used P[ix]\n",
    "        # p = P[ix]\n",
    "\n",
    "        # now we use the softmax of the logits\n",
    "        xenc = F.one_hot(torch.tensor([ix1, ix2]).to(device), num_classes = 27).float().to(device)\n",
    "        xenc = xenc.view(-1, 27*2)\n",
    "        \n",
    "        logits = xenc @ W\n",
    "        counts = torch.exp(logits)\n",
    "        p = counts / counts.sum(dim = 1, keepdim = True)\n",
    "\n",
    "        ix1 = ix2\n",
    "        ix2 = torch.multinomial(p.to(device), num_samples = 1 , replacement = True).item()\n",
    "        out.append(itos[ix2])\n",
    "        if ix2 == 0:\n",
    "            break\n",
    "\n",
    "    names.append(\"\".join(out))\n",
    "    \n",
    "for name in names:\n",
    "    print(name)\n",
    "count_loss(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe4c967-9e0b-4e33-87da-3ae5775acd0e",
   "metadata": {},
   "source": [
    "##  Ex-2 split up the dataset randomly into 80% train set, 10% dev set, 10% test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e6bd25d0-301e-4612-8628-d8c88c81ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "words_train, words_test = train_test_split(words, test_size=0.2, random_state=1234)\n",
    "words_dev, words_test = train_test_split(words_test, test_size=0.5, random_state=1234)\n",
    "\n",
    "x_train, y_train, x_dev, y_dev, x_test, y_test = [], [], [], [], [], []\n",
    "for wgroup in [words_train, words_dev, words_test]:\n",
    "    xs , ys = [], []\n",
    "    for w in wgroup:\n",
    "        # add start and end tokens\n",
    "        chs = [\".\"] + list(w) + [\".\"]\n",
    "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            ix1 = stoi[ch1]\n",
    "            ix2 = stoi[ch2]\n",
    "            ix3 = stoi[ch3]\n",
    "        \n",
    "            xs.append([ix1, ix2])\n",
    "            ys.append(ix3)\n",
    "\n",
    "    xs = torch.tensor(xs, dtype=torch.int64)\n",
    "    ys = torch.tensor(ys, dtype=torch.int64)\n",
    "\n",
    "    if wgroup == words_train:\n",
    "        x_train, y_train = xs, ys\n",
    "    elif wgroup == words_dev:\n",
    "        x_dev, y_dev = xs, ys\n",
    "    else:\n",
    "        x_test, y_test = xs, ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a8b2852f-3c91-4bb5-97d5-7a48b0998ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 4.3494\n",
      "10: 2.4803\n",
      "20: 2.3664\n",
      "30: 2.3256\n",
      "40: 2.3041\n",
      "50: 2.2907\n",
      "60: 2.2814\n",
      "70: 2.2747\n",
      "80: 2.2695\n",
      "90: 2.2654\n",
      "100: 2.2622\n",
      "110: 2.2595\n",
      "120: 2.2573\n",
      "130: 2.2554\n",
      "140: 2.2538\n",
      "150: 2.2524\n",
      "160: 2.2512\n",
      "170: 2.2502\n",
      "180: 2.2493\n",
      "190: 2.2484\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((27*2,27), requires_grad = True, device = device)\n",
    "for k in range(200):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(x_train, num_classes = 27).float().to(device)\n",
    "    xenc = xenc.view(-1, 27*2)\n",
    "    \n",
    "    # probs is softmax\n",
    "    logits = xenc @ W\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / counts.sum(dim = 1, keepdim = True)\n",
    "    \n",
    "    # loss (normalized negative log likelihood)\n",
    "    loss = - probs[torch.arange(len(x_train)), y_train].log().mean()\n",
    "    # add regularization\n",
    "    # loss += 0.2 * W.pow(2).mean()\n",
    "\n",
    "    if k % 10 == 0:\n",
    "        print(f\"{k}: {loss.item():.4f}\")\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        W -= 50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b9f704ee-3847-46c3-b244-145753a9ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_loss(x, y, W):\n",
    "    xenc = F.one_hot(x, num_classes = 27).float().to(device)\n",
    "    xenc = xenc.view(-1, 27*2)\n",
    "\n",
    "    # probs is softmax\n",
    "    logits = xenc @ W\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / counts.sum(dim = 1, keepdim = True)\n",
    "\n",
    "    # loss (normalized negative log likelihood)\n",
    "    loss = - probs[torch.arange(len(x)), y].log().mean()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1143b2b9-f7d5-4e59-a924-85e69ff886b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.2477\n",
      "Dev Loss: 2.2525\n",
      "Test Loss: 2.2510\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Loss: {MLP_loss(x_train, y_train, W):.4f}\")\n",
    "print(f\"Dev Loss: {MLP_loss(x_dev, y_dev, W):.4f}\")\n",
    "print(f\"Test Loss: {MLP_loss(x_test, y_test, W):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56476476-5fa6-4a87-a449-a03356233cbc",
   "metadata": {},
   "source": [
    "## Ex-3: Use Dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0c2ba0b6-025f-42f4-ba95-42782bb3cc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Train Loss: 4.2443 | Dev Loss 4.2576\n",
      "10: Train Loss: 2.5107 | Dev Loss 2.5157\n",
      "20: Train Loss: 2.3791 | Dev Loss 2.3816\n",
      "30: Train Loss: 2.3313 | Dev Loss 2.3335\n",
      "40: Train Loss: 2.3069 | Dev Loss 2.3093\n",
      "50: Train Loss: 2.2923 | Dev Loss 2.2950\n",
      "60: Train Loss: 2.2826 | Dev Loss 2.2856\n",
      "70: Train Loss: 2.2756 | Dev Loss 2.2789\n",
      "80: Train Loss: 2.2704 | Dev Loss 2.2739\n",
      "90: Train Loss: 2.2664 | Dev Loss 2.2701\n",
      "100: Train Loss: 2.2631 | Dev Loss 2.2670\n",
      "110: Train Loss: 2.2604 | Dev Loss 2.2645\n",
      "120: Train Loss: 2.2582 | Dev Loss 2.2625\n",
      "130: Train Loss: 2.2563 | Dev Loss 2.2607\n",
      "140: Train Loss: 2.2547 | Dev Loss 2.2592\n",
      "150: Train Loss: 2.2533 | Dev Loss 2.2579\n",
      "160: Train Loss: 2.2520 | Dev Loss 2.2568\n",
      "170: Train Loss: 2.2509 | Dev Loss 2.2558\n",
      "180: Train Loss: 2.2500 | Dev Loss 2.2549\n",
      "190: Train Loss: 2.2491 | Dev Loss 2.2542\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((27*2,27), requires_grad = True, device = device)\n",
    "for k in range(200):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(x_train, num_classes = 27).float().to(device)\n",
    "    xenc = xenc.view(-1, 27*2)\n",
    "    \n",
    "    # probs is softmax\n",
    "    logits = xenc @ W\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / counts.sum(dim = 1, keepdim = True)\n",
    "    \n",
    "    # loss (normalized negative log likelihood)\n",
    "    loss = - probs[torch.arange(len(x_train)), y_train].log().mean()\n",
    "    # add regularization\n",
    "    # loss += 0.05 * W.pow(2).mean()\n",
    "\n",
    "    if k % 10 == 0:\n",
    "        print(f\"{k}: Train Loss: {loss.item():.4f} | Dev Loss {MLP_loss(x_dev, y_dev, W):.4f}\")\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        W -= 50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2ef0fb-192d-4d0c-9ff8-4d01fa513edb",
   "metadata": {},
   "source": [
    "No regularization is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5239320d-2b9b-4c4e-ae3b-482adb974566",
   "metadata": {},
   "source": [
    "## Ex-4: Rewrite the MLP model without creating one hot vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a0f4d9ac-76b0-4a80-93d7-9ffdc52368c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 4.1857\n",
      "10: 2.5815\n",
      "20: 2.4542\n",
      "30: 2.4075\n",
      "40: 2.3847\n",
      "50: 2.3720\n",
      "60: 2.3643\n",
      "70: 2.3595\n",
      "80: 2.3564\n",
      "90: 2.3544\n",
      "100: 2.3530\n",
      "110: 2.3520\n",
      "120: 2.3514\n",
      "130: 2.3509\n",
      "140: 2.3506\n",
      "150: 2.3504\n",
      "160: 2.3503\n",
      "170: 2.3501\n",
      "180: 2.3501\n",
      "190: 2.3500\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((27*2,27), requires_grad = True, device = device)\n",
    "for k in range(200):\n",
    "    # forward pass\n",
    "    # ====================\n",
    "    # Previously: using onehot and multiplying by W \n",
    "    # xenc = F.one_hot(xs, num_classes = 27).float().to(device)\n",
    "    # xenc = xenc.view(-1, 27*2)\n",
    "    # logits = xenc @ W\n",
    "    # ====================\n",
    "\n",
    "    # ====================\n",
    "    # ✅ now: acess by xs indices directly\n",
    "    logits = W[xs[:,0]] + W[xs[:,1] + 27]\n",
    "    # ====================\n",
    "    \n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / counts.sum(dim = 1, keepdim = True)\n",
    "    \n",
    "    # loss (normalized negative log likelihood)\n",
    "    loss = - probs[torch.arange(len(xs)), ys].log().mean()\n",
    "    # add regularization\n",
    "    loss += 0.2 * W.pow(2).mean()\n",
    "\n",
    "    if k % 10 == 0:\n",
    "        print(f\"{k}: {loss.item():.4f}\")\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        W -= 50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cfb65a-40b3-48b8-8a25-a974098176d5",
   "metadata": {},
   "source": [
    "# Ex-5: look up and use F.cross_entropy instead\n",
    "nn.functonal.cross_entropy() takes the logits and the target class as input and returns the cross entropy loss directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "cfc5e617-ba44-4c8f-9f65-203aa192e53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 4.3923\n",
      "10: 2.6107\n",
      "20: 2.4677\n",
      "30: 2.4140\n",
      "40: 2.3879\n",
      "50: 2.3736\n",
      "60: 2.3651\n",
      "70: 2.3600\n",
      "80: 2.3567\n",
      "90: 2.3545\n",
      "100: 2.3531\n",
      "110: 2.3521\n",
      "120: 2.3514\n",
      "130: 2.3510\n",
      "140: 2.3507\n",
      "150: 2.3504\n",
      "160: 2.3503\n",
      "170: 2.3501\n",
      "180: 2.3501\n",
      "190: 2.3500\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((27*2,27), requires_grad = True, device = device)\n",
    "for k in range(200):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes = 27).float().to(device)\n",
    "    xenc = xenc.view(-1, 27*2)\n",
    "    logits = xenc @ W\n",
    "    \n",
    "    loss = torch.nn.functional.cross_entropy(logits, ys.to(device))\n",
    "    # add regularization\n",
    "    loss += 0.2 * W.pow(2).mean()\n",
    "\n",
    "    if k % 10 == 0:\n",
    "        print(f\"{k}: {loss.item():.4f}\")\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        W -= 50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f3d59-3ec0-481a-829f-828e229ebb93",
   "metadata": {},
   "source": [
    "# Ex-6: meta-exercise! Think of a fun/interesting exercise and complete it\n",
    "more % in dev and more iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "bfd6ff47-74be-4fc6-a685-76fa2d336df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "words_train, words_test = train_test_split(words, test_size=0.2, random_state=1234)\n",
    "words_dev, words_test = train_test_split(words_test, test_size=0.3, random_state=1234)\n",
    "\n",
    "x_train, y_train, x_dev, y_dev, x_test, y_test = [], [], [], [], [], []\n",
    "for wgroup in [words_train, words_dev, words_test]:\n",
    "    xs , ys = [], []\n",
    "    for w in wgroup:\n",
    "        # add start and end tokens\n",
    "        chs = [\".\"] + list(w) + [\".\"]\n",
    "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            ix1 = stoi[ch1]\n",
    "            ix2 = stoi[ch2]\n",
    "            ix3 = stoi[ch3]\n",
    "        \n",
    "            xs.append([ix1, ix2])\n",
    "            ys.append(ix3)\n",
    "\n",
    "    xs = torch.tensor(xs, dtype=torch.int64)\n",
    "    ys = torch.tensor(ys, dtype=torch.int64)\n",
    "\n",
    "    if wgroup == words_train:\n",
    "        x_train, y_train = xs, ys\n",
    "    elif wgroup == words_dev:\n",
    "        x_dev, y_dev = xs, ys\n",
    "    else:\n",
    "        x_test, y_test = xs, ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "56c9d1d0-7355-47f4-9a10-a4259d06f03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 4.3736\n",
      "10: 2.4763\n",
      "20: 2.3649\n",
      "30: 2.3238\n",
      "40: 2.3022\n",
      "50: 2.2889\n",
      "60: 2.2798\n",
      "70: 2.2733\n",
      "80: 2.2684\n",
      "90: 2.2645\n",
      "100: 2.2615\n",
      "110: 2.2590\n",
      "120: 2.2569\n",
      "130: 2.2551\n",
      "140: 2.2536\n",
      "150: 2.2523\n",
      "160: 2.2512\n",
      "170: 2.2501\n",
      "180: 2.2492\n",
      "190: 2.2484\n",
      "200: 2.2477\n",
      "210: 2.2471\n",
      "220: 2.2465\n",
      "230: 2.2459\n",
      "240: 2.2455\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((27*2,27), requires_grad = True, device = device)\n",
    "for k in range(250):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(x_train, num_classes = 27).float().to(device)\n",
    "    xenc = xenc.view(-1, 27*2)\n",
    "    \n",
    "    # probs is softmax\n",
    "    logits = xenc @ W\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / counts.sum(dim = 1, keepdim = True)\n",
    "    \n",
    "    # loss (normalized negative log likelihood)\n",
    "    loss = - probs[torch.arange(len(x_train)), y_train].log().mean()\n",
    "    # add regularization\n",
    "    # loss += 0.2 * W.pow(2).mean()\n",
    "\n",
    "    if k % 10 == 0:\n",
    "        print(f\"{k}: {loss.item():.4f}\")\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        W -= 50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271a996f-d08a-4cc5-9cd6-f2b696f623e4",
   "metadata": {},
   "source": [
    "for learning rates we used [0.1, 0.05, 0.05, 0.001] 30k steps each"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
